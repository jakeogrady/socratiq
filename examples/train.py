

from __future__ import annotations

import os
from pathlib import Path

import torch
from datasets import load_dataset
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
)

DATASET_ID = "nyamuda/samsum"
BASE_MODEL_ID = os.getenv("BASE_MODEL_ID", "google-t5/t5-small")
HUB_MODEL_ID = os.getenv("FINETUNED_MODEL_ID", "Jakeog123/google-t5-samsum-demo")
OUTPUT_DIR = Path("flan_samsum_demo")

TRAIN_EXAMPLES = int(os.getenv("TRAIN_EXAMPLES", "1000"))
VAL_EXAMPLES = int(os.getenv("VAL_EXAMPLES", "0"))
NUM_EPOCHS = float(os.getenv("NUM_EPOCHS", "3"))
LEARNING_RATE = float(os.getenv("LEARNING_RATE", "2e-5"))
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "4"))
GRADIENT_ACCUMULATION = int(os.getenv("GRAD_ACCUM", "8"))
WARMUP_RATIO = float(os.getenv("WARMUP_RATIO", "0.05"))
LABEL_SMOOTHING = float(os.getenv("LABEL_SMOOTHING", "0.1"))


def ensure_model_card(path: Path) -> None:
    """Prefix the autogenerated model card with pipeline metadata."""

    header = (
        "---\n"
        "pipeline_tag: summarization\n"
        "tags:\n"
        "  - t5\n"
        "  - summarization\n"
        "  - text2text-generation\n"
        "---\n\n"
    )

    if path.exists():
        content = path.read_text()
        if content.lstrip().startswith("---"):
            return
        path.write_text(f"{header}{content}")
    else:
        path.write_text(header)


def main() -> None:
    dataset = load_dataset(DATASET_ID)
    train_slice = dataset["train"]
    val_slice = dataset["validation"]

    if TRAIN_EXAMPLES:
        train_slice = train_slice.select(range(min(TRAIN_EXAMPLES, len(train_slice))))
    if VAL_EXAMPLES:
        val_slice = val_slice.select(range(min(VAL_EXAMPLES, len(val_slice))))

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
    model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_ID)

    prefix = os.getenv("TASK_PREFIX", "summarize: ")
    pad_token_id = tokenizer.pad_token_id

    def preprocess(batch):
        inputs = [prefix + dialog for dialog in batch["dialogue"]]
        x = tokenizer(
            inputs,
            max_length=256,
            truncation=True,
            padding="max_length",
        )
        y = tokenizer(
            batch["summary"],
            max_length=128,
            truncation=True,
            padding="max_length",
        )
        labels = []
        for row in y["input_ids"]:
            cleaned = [(token if token != pad_token_id else -100) for token in row]
            labels.append(cleaned)
        x["labels"] = labels
        return x

    train_ds = train_slice.map(preprocess, batched=True, remove_columns=train_slice.column_names)
    val_ds = val_slice.map(preprocess, batched=True, remove_columns=val_slice.column_names)

    collator = DataCollatorForSeq2Seq(tokenizer, model=model)
    use_fp16 = torch.cuda.is_available()

    args = Seq2SeqTrainingArguments(
        output_dir=str(OUTPUT_DIR),
        eval_strategy="epoch",
        learning_rate=LEARNING_RATE,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION,
        num_train_epochs=NUM_EPOCHS,
        weight_decay=0.01,
        predict_with_generate=True,
        fp16=use_fp16,
        push_to_hub=True,
        hub_model_id=HUB_MODEL_ID,
        warmup_ratio=WARMUP_RATIO,
        label_smoothing_factor=LABEL_SMOOTHING,
        generation_max_length=128,
        save_total_limit=2,
        logging_steps=50,
        report_to=["none"],
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        tokenizer=tokenizer,
        data_collator=collator,
    )

    trainer.train()

    ensure_model_card(OUTPUT_DIR / "README.md")


if __name__ == "__main__":
    main()